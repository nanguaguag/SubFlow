import argparse
import os
import json
import whisper
import torch
from pathlib import Path
# å¼•å…¥åŸæœ‰æ¨¡å—
from subtitle.translator import OpenAITranslator
from subtitle.subtitle_core import WhisperPostProcessor, SRTExporter, ASSExporter
# å¼•å…¥æ–°çš„éŸ³ä¹æ¨¡å—
from subtitle.music_core import AudioConverter, WhisperLyricProcessor, LRCExporter, EnhancedLRCExporter, KaraokeASSExporter

# --------------------------------------------
# ä¸»æµç¨‹
# --------------------------------------------


def main():
    p = argparse.ArgumentParser(
        description="Auto Subtitle Generator for Anime & Songs")

    p.add_argument(
        "input", help="Input audio/video file path (mp3/mp4/mkv/wav...)")

    # æ¨¡å¼é€‰æ‹©
    p.add_argument("--mode", choices=["anime", "song"], default="anime",
                   help="Processing mode: 'anime' for dialogue, 'song' for karaoke lyrics")
    p.add_argument("-m", "--model", default="medium",
                   help="Whisper model: tiny/base/small/medium/large/turbo")

    # ç¿»è¯‘ç›¸å…³
    p.add_argument("-l", "--language", default="ja",
                   help="Language code, e.g. ja, en, zh")
    p.add_argument("--translate", action="store_true",
                   help="Enable JP->CN translation (Mock)")
    p.add_argument("--api_key", default=None, help="OpenAI API Key")
    p.add_argument("--base_url", default=None,
                   help="OpenAI Base URL (optional)")
    p.add_argument("--gpt_model", default="gpt-4o-mini",
                   help="LLM model name (default: gpt-4.1-mini)")  # é»˜è®¤ç”¨ mini

    # å­—å¹•æ ·å¼ç›¸å…³
    p.add_argument("--sub_style", choices=["bilingual", "zh", "jp"], default="bilingual",
                   help="Subtitle style: bilingual (default), zh, or jp")

    # è¾“å‡ºç›¸å…³
    p.add_argument("--out_dir", default=None, help="Output directory")
    p.add_argument("--srt", action="store_true", help="Generate .srt")
    p.add_argument("--ass", action="store_true", help="Generate .ass")

    # Whisper è½¬å†™å‚æ•°
    p.add_argument("--device", default=None,
                   help="Force device: cpu or mps (default: whisper auto)")
    p.add_argument("--beam_size", type=int, default=5)
    p.add_argument("--best_of", type=int, default=5)
    p.add_argument("--temperature", type=float, default=0.0)
    p.add_argument("--no_speech_threshold", type=float, default=0.6)
    p.add_argument("--condition_on_previous_text",
                   action="store_true", default=False)
    p.add_argument("--min_gap", type=float, default=0.25,
                   help="Split segments if gap >= this seconds")
    p.add_argument("--max_gap", type=float, default=0.02,
                   help="Merge segments if gap <= this seconds")
    p.add_argument("--max_merged_duration", type=float,
                   default=7.0, help="Max duration after merging")
    p.add_argument("--max_chars", type=int, default=18,
                   help="Max chars per line")
    p.add_argument("--max_lines", type=int, default=2,
                   help="Max lines per subtitle")

    # ASS æ ·å¼
    p.add_argument("--play_res_x", type=int, default=1920)
    p.add_argument("--play_res_y", type=int, default=1080)
    p.add_argument("--font", type=str, default="Noto Sans CJK SC")
    p.add_argument("--font_size", type=int, default=54)

    args = p.parse_args()

    # 1. è·¯å¾„å¤„ç† (ä½¿ç”¨ Pathlib æ›´ä¼˜é›…)
    input_path = Path(args.input)
    if not input_path.exists():
        raise FileNotFoundError(f"Input not found: {input_path}")

    out_dir = Path(args.out_dir) if args.out_dir else input_path.parent
    base_name = input_path.stem

    if args.mode not in ["song", "anime"]:
        print(f"ä¸æ”¯æŒçš„å‚æ•°: --mode {args.mode}")
        print(f"å½“å‰åªæ”¯æŒåŠ¨æ¼«å’ŒéŸ³ä¹ï¼š--mode anime æˆ– --mode song")
        return

    # ------------------------------------------------------
    # åˆ†æ”¯ A: æ­Œæ›²æ¨¡å¼ (Song Mode)
    # ------------------------------------------------------
    if args.mode == "song":
        print("ğŸµ Entering Song/Karaoke Mode...")

        # 1. éŸ³é¢‘è½¬æ¢ (è½¬ä¸º m4a)
        m4a_path = AudioConverter.convert_to_m4a(input_path, out_dir)

        # 2. Whisper è½¬å†™ (å¼ºåˆ¶å¼€å¯ word_timestamps)
        raw_json_path = out_dir / f"{base_name}_song_raw.json"

        if raw_json_path.exists():
            print("ğŸ“‚ Found existing raw JSON, skipping Whisper...")
            with open(raw_json_path, "r", encoding="utf-8") as f:
                result = json.load(f)
        else:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸš€ Loading Whisper model '{args.model}' on {device}...")
            model = whisper.load_model(args.model, device=device)

            print("ğŸ™ï¸ Transcribing song (word-level)...")
            result = model.transcribe(
                str(m4a_path),
                language=args.language,
                word_timestamps=True,  # æ­Œæ›²æ¨¡å¼å¼ºåˆ¶å¼€å¯
                initial_prompt="Lyrics of a song. æ­Œè©ã€‚"
            )
            with open(raw_json_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

        # 3. å¤„ç†ä¸ºæ­Œè¯è¡Œç»“æ„
        print("âœ‚ï¸ Processing lyrics...")
        lyric_processor = WhisperLyricProcessor()
        lyric_lines = lyric_processor.process(result)
        print(f"âœ… Generated {len(lyric_lines)} lyric lines.")

        # 4. (é¢„ç•™) ç¿»è¯‘ - æš‚æ—¶ç•™ç©ºæˆ–ä»…åšç®€å•æ‹·è´
        if args.translate:
            print("âš ï¸ Translation for song mode is not yet implemented (Coming soon).")
            # æœªæ¥åœ¨è¿™é‡Œè°ƒç”¨ translatorï¼Œé’ˆå¯¹ LyricLine è¿›è¡Œç¿»è¯‘

        # 5. å¯¼å‡ºæ–‡ä»¶
        print("ğŸ’¾ Saving song subtitles...")

        # å¯¼å‡º LRC
        lrc_path = out_dir / f"{base_name}.lrc"
        LRCExporter().export(lyric_lines, str(lrc_path))
        print(f"  -> {lrc_path}")

        # å¯¼å‡º Enhanced LRC
        lrc_path = out_dir / f"{base_name}_e.lrc"
        EnhancedLRCExporter().export(lyric_lines, str(lrc_path))
        print(f"  -> {lrc_path}")

        # å¯¼å‡º Karaoke ASS
        ass_path = out_dir / f"{base_name}_k.ass"
        KaraokeASSExporter().export(lyric_lines, str(ass_path))
        print(f"  -> {ass_path}")

        print("âœ¨ Song processing done!")
        return

    # ------------------------------------------------------
    # åˆ†æ”¯ B: åŠ¨æ¼«/å¯¹è¯æ¨¡å¼ (Anime Mode - åŸæœ‰é€»è¾‘)
    # ------------------------------------------------------
    # ... (ä¿æŒåŸæœ‰çš„ Anime é€»è¾‘ä»£ç ä¸å˜ï¼Œæˆ–è€…å°è£…æˆå‡½æ•°è°ƒç”¨) ...
    # ä¸‹é¢æ˜¯åŸæœ‰é€»è¾‘çš„ç®€åŒ–ç‰ˆï¼Œä½ å¯ä»¥ç›´æ¥æŠŠåŸæœ‰ä»£ç æ”¾åœ¨ else å—é‡Œ

    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç”Ÿæˆçš„ raw.jsonï¼Œå¦‚æœæœ‰å°±è·³è¿‡ whisperï¼Œæ–¹ä¾¿è°ƒè¯•ç¿»è¯‘
    raw_json_path = out_dir / f"{base_name}_raw.json"

    if raw_json_path.exists():
        print("ğŸ“‚ Found existing raw JSON, skipping Whisper...")
        with open(raw_json_path, "r", encoding="utf-8") as f:
            result = json.load(f)
    else:
        # 2. åŠ è½½æ¨¡å‹
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ Loading Whisper model '{args.model}' on {device}...")
        model = whisper.load_model(args.model, device=device)

        # 3. æ‰§è¡Œè½¬å†™ (STT)
        print("ğŸ™ï¸ Transcribing anime dialogue...")
        # å»ºè®®åŠ ä¸Š initial_prompt æç¤ºæ˜¯åŠ¨æ¼«
        result = model.transcribe(
            str(input_path),
            language=args.language,
            word_timestamps=True,  # å…³é”®ï¼šå¼€å¯è¯çº§æ—¶é—´æˆ³ä»¥è·å¾—æ›´å¥½åˆ‡åˆ†
            initial_prompt="ã‚¢ãƒ‹ãƒ¡ã®æ—¥æœ¬èªå­—å¹•ã€‚å¸¸ä½“ã€‚å£èªã€‚"
        )
        # ä¿å­˜ä¸­é—´ç»“æœ
        with open(raw_json_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

    # 4. åå¤„ç†ï¼šæ¸…æ´—ä¸åˆ‡åˆ†
    print("âœ‚ï¸ Processing segments...")
    processor = WhisperPostProcessor(use_word_timestamps=True)
    events = processor.process(result, split_gap=args.min_gap)

    # # åˆå¹¶è¿‡ç¢ç‰‡æ®µ
    # # max_gap=0.02: åªæœ‰å½“ä¸¤æ¡å­—å¹•ä¸­é—´çš„ç¼éš™å°äº 0.02 ç§’ æ—¶æ‰åˆå¹¶ï¼ˆå‡ ä¹æ˜¯è¿ç€è¯»ï¼‰
    # events = processor.merge_nearby(events, max_gap=max_gap)
    # print(f"âœ… Generated {len(events)} subtitle events.")

    # 5. ç¿»è¯‘æ¨¡å— (LLM)
    if args.translate:
        api_key = args.api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âš ï¸ Warning: No API Key provided. Skipping translation.")
        else:
            print(f"ğŸ¤– Translating via {args.gpt_model}")
            print(f"â†’ Translation style: {args.sub_style})...")

            translator = OpenAITranslator(
                api_key=api_key,
                base_url=args.base_url,
                model=args.gpt_model
            )
            # æ‰§è¡Œç¿»è¯‘
            translator.translate_events(events)

    # 6. è®¾ç½®å­—å¹•æ˜¾ç¤ºæ¨¡å¼
    print(f"ğŸ¨ Applying subtitle style: {args.sub_style}")
    for ev in events:
        ev.render_mode = args.sub_style

    # 7. å¯¼å‡ºæ–‡ä»¶
    print("ğŸ’¾ Saving anime subtitles...")

    # å¯¼å‡º SRT
    srt_path = out_dir / f"{base_name}.srt"
    SRTExporter().export(events, str(srt_path))
    print(f"ğŸ’¾ Saved: {srt_path}")

    # å¯¼å‡º ASS
    ass_path = out_dir / f"{base_name}.ass"
    ASSExporter().export(events, str(ass_path))
    print(f"ğŸ’¾ Saved: {ass_path}")

    print("âœ¨ Anime processing done!")


if __name__ == "__main__":
    main()
