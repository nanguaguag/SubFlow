import argparse
import os
import json
import whisper
import torch
from pathlib import Path
from subtitle.translator import OpenAITranslator
from subtitle.subtitle_core import WhisperPostProcessor, SRTExporter, ASSExporter

# --------------------------------------------
# ä¸»æµç¨‹
# --------------------------------------------


def main():
    p = argparse.ArgumentParser(
        description="Auto Anime Subtitle Generator -> generate SRT/ASS subtitles (anime-friendly basic formatting)"
    )
    p.add_argument(
        "input", help="Input audio/video file path (mp3/mp4/mkv/wav...)")
    p.add_argument("-m", "--model", default="medium",
                   help="Whisper model: tiny/base/small/medium/large/turbo")

    # ç¿»è¯‘ç›¸å…³
    p.add_argument("-l", "--language", default="ja",
                   help="Language code, e.g. ja, en, zh")
    p.add_argument("--translate", action="store_true",
                   help="Enable JP->CN translation (Mock)")
    p.add_argument("--api_key", default=None, help="OpenAI API Key")
    p.add_argument("--base_url", default=None,
                   help="OpenAI Base URL (optional)")
    p.add_argument("--gpt_model", default="gpt-4o-mini",
                   help="LLM model name (default: gpt-4.1-mini)")  # é»˜è®¤ç”¨ mini

    # å­—å¹•æ ·å¼ç›¸å…³
    p.add_argument("--sub_style", choices=["bilingual", "zh", "jp"], default="bilingual",
                   help="Subtitle style: bilingual (default), zh, or jp")

    # è¾“å‡ºç›¸å…³
    p.add_argument("--out_dir", default=None, help="Output directory")
    p.add_argument("--srt", action="store_true", help="Generate .srt")
    p.add_argument("--ass", action="store_true", help="Generate .ass")

    # Whisper è½¬å†™å‚æ•°
    p.add_argument("--device", default=None,
                   help="Force device: cpu or mps (default: whisper auto)")
    p.add_argument("--beam_size", type=int, default=5)
    p.add_argument("--best_of", type=int, default=5)
    p.add_argument("--temperature", type=float, default=0.0)
    p.add_argument("--no_speech_threshold", type=float, default=0.6)
    p.add_argument("--condition_on_previous_text",
                   action="store_true", default=False)
    p.add_argument("--max_gap", type=float, default=0.25,
                   help="Merge segments if gap <= this seconds")
    p.add_argument("--max_merged_duration", type=float,
                   default=7.0, help="Max duration after merging")
    p.add_argument("--max_chars", type=int, default=18,
                   help="Max chars per line")
    p.add_argument("--max_lines", type=int, default=2,
                   help="Max lines per subtitle")

    # ASS æ ·å¼
    p.add_argument("--play_res_x", type=int, default=1920)
    p.add_argument("--play_res_y", type=int, default=1080)
    p.add_argument("--font", type=str, default="Noto Sans CJK SC")
    p.add_argument("--font_size", type=int, default=54)

    args = p.parse_args()

    # 1. è·¯å¾„å¤„ç† (ä½¿ç”¨ Pathlib æ›´ä¼˜é›…)
    input_path = Path(args.input)
    if not input_path.exists():
        raise FileNotFoundError(f"Input not found: {input_path}")

    out_dir = Path(args.out_dir) if args.out_dir else input_path.parent
    base_name = input_path.stem

    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç”Ÿæˆçš„ raw.jsonï¼Œå¦‚æœæœ‰å°±è·³è¿‡ whisperï¼Œæ–¹ä¾¿è°ƒè¯•ç¿»è¯‘
    raw_json_path = out_dir / f"{base_name}_raw.json"

    if raw_json_path.exists():
        print("ğŸ“‚ Found existing raw JSON, skipping Whisper...")
        with open(raw_json_path, "r", encoding="utf-8") as f:
            result = json.load(f)
    else:
        # 2. åŠ è½½æ¨¡å‹
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ Loading Whisper model '{args.model}' on {device}...")
        model = whisper.load_model(args.model, device=device)

        # 3. æ‰§è¡Œè½¬å†™ (STT)
        print("ğŸ™ï¸ Transcribing audio...")
        # å»ºè®®åŠ ä¸Š initial_prompt æç¤ºæ˜¯åŠ¨æ¼«
        result = model.transcribe(
            str(input_path),
            language="ja",
            word_timestamps=True,  # å…³é”®ï¼šå¼€å¯è¯çº§æ—¶é—´æˆ³ä»¥è·å¾—æ›´å¥½åˆ‡åˆ†
            beam_size=5,
            initial_prompt="ã‚¢ãƒ‹ãƒ¡ã®æ—¥æœ¬èªå­—å¹•ã€‚å¸¸ä½“ã€‚å£èªã€‚"
        )
        # ä¿å­˜ä¸­é—´ç»“æœ
        with open(raw_json_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

    # 4. åå¤„ç†ï¼šæ¸…æ´—ä¸åˆ‡åˆ†
    print("âœ‚ï¸ Processing segments...")
    processor = WhisperPostProcessor(use_word_timestamps=True)
    events = processor.process(result, split_gap=0.25)

    # # åˆå¹¶è¿‡ç¢ç‰‡æ®µ
    # max_gap=0.1: åªæœ‰å½“ä¸¤æ¡å­—å¹•ä¸­é—´çš„ç¼éš™å°äº 0.1ç§’ æ—¶æ‰åˆå¹¶ï¼ˆå‡ ä¹æ˜¯è¿ç€è¯»ï¼‰
    # events = processor.merge_nearby(events, max_gap=0.1)
    # print(f"âœ… Generated {len(events)} subtitle events.")

    # 5. ç¿»è¯‘æ¨¡å— (LLM)
    if args.translate:
        api_key = args.api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âš ï¸ Warning: No API Key provided. Skipping translation.")
        else:
            print(
                f"ğŸ¤– Translating via {args.gpt_model} (Style: {args.sub_style})...")
            translator = OpenAITranslator(
                api_key=api_key,
                base_url=args.base_url,
                model=args.gpt_model
            )
            # æ‰§è¡Œç¿»è¯‘
            translator.translate_events(events)

    # 6. è®¾ç½®å­—å¹•æ˜¾ç¤ºæ¨¡å¼
    print(f"ğŸ¨ Applying subtitle style: {args.sub_style}")
    for ev in events:
        ev.render_mode = args.sub_style

    # 7. å¯¼å‡ºæ–‡ä»¶
    print("ğŸ’¾ Saving files...")

    # å¯¼å‡º SRT
    srt_path = out_dir / f"{base_name}.srt"
    SRTExporter().export(events, str(srt_path))
    print(f"ğŸ’¾ Saved: {srt_path}")

    # å¯¼å‡º ASS
    ass_path = out_dir / f"{base_name}.ass"
    ASSExporter().export(events, str(ass_path))
    print(f"ğŸ’¾ Saved: {ass_path}")

    print("âœ¨ All done!")
if __name__ == "__main__":
    main()
